{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iBxWaQId_uW"
      },
      "source": [
        "Реализовано раздельное детектирование и треккинг объектов по уже готовым массивам из детекции, что позволяет подбирать архитектуру трекера и постобработки изолировано, без необходимости запуска предикта YOLO (ускорение подбора треккера и постобработки примерно в 100 раз и без траты лимита GPU в колаб) - также потенциально возможна генетика архитектуры постобработки и треккинга. Кроме того, реализован проброс касок и жилетов мимо трекера, что позволяет лучше трековать людей, а также не приводит к фильтрации касок и жилетов трекером, это позволяет лучше детектировать нарушения, избегать ложных фиксаций нарушений. Также реализовано восстановление в массиве информации нетрекованных боксов людей (которые трекер отфильтровал, отбросил).\n",
        "В постобработке реализовано два подхода по подсчету людей а также вероятностный подход фиксации нарушений. Под вероятностным понимается отношение количества кадров в которых конкретный айди находится в каске и жилете к общему количеству кадров с этим айди (отношение больше минимального порога - нарушения нет)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF38gPBa_TgF"
      },
      "source": [
        "## Загрузка датасэта"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JilFHfxd9ci",
        "outputId": "8a35bc75-9845-4c11-efa6-3771b6c0aedf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive           # у кого автоматически не подключается гугл диск\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "2Ob9OdsRvlqj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AxEPLFb_Mt4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5182b595-80d6-4603-a585-2ffdb9515e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.78-py3-none-any.whl (513 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.5/513.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (8.4.0)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.0.0+cu118)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.12.2)\n",
            "Collecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.27.1)\n",
            "Collecting sentry-sdk\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.65.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ultralytics) (5.9.4)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.15.1+cu118)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.11.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7.0->ultralytics) (3.25.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: sentry-sdk, thop, ultralytics\n",
            "Successfully installed sentry-sdk-1.19.1 thop-0.1.1.post2209072238 ultralytics-8.0.78\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting filterpy\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from filterpy) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from filterpy) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from filterpy) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (8.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (5.12.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->filterpy) (4.39.3)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->filterpy) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n",
            "Building wheels for collected packages: filterpy\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110473 sha256=ae95b06e09aa20ad2c62ac1120709709470efc95c0c47adc68b6905b2bce79f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/e6/de/a09ea01e923aaf88b9f8c7c44329e857b2c1a31901167e55e6\n",
            "Successfully built filterpy\n",
            "Installing collected packages: filterpy\n",
            "Successfully installed filterpy-1.4.5\n",
            "Cloning into 'OC_SORT'...\n",
            "remote: Enumerating objects: 493, done.\u001b[K\n",
            "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 493 (delta 44), reused 74 (delta 42), pack-reused 409\u001b[K\n",
            "Receiving objects: 100% (493/493), 43.28 MiB | 22.29 MiB/s, done.\n",
            "Resolving deltas: 100% (161/161), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install filterpy\n",
        "!git clone https://github.com/noahcao/OC_SORT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/OC_SORT\n",
        "from trackers.ocsort_tracker.ocsort import OCSort"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_E9PYPDiHcX",
        "outputId": "bd7071fe-f6be-441a-c2dd-964cce0b3cf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/OC_SORT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/belouspavel/yolov8.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_M79qTdgETx",
        "outputId": "14da56f7-a0f8-4ba1-86c8-a1c51846bb26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'yolov8'...\n",
            "remote: Enumerating objects: 572, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 572 (delta 27), reused 67 (delta 17), pack-reused 493\u001b[K\n",
            "Receiving objects: 100% (572/572), 86.65 MiB | 30.28 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/yolov8\n",
        "from yolov8_tools import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1aXA64wkHhx",
        "outputId": "2d84107b-3c9c-4b13-9aee-336191abdc3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrWoKy00oCWj"
      },
      "source": [
        "## Обучение и сохранение детекции по набору видео\n",
        "Эту часть кода можно (нужно) выполнить только один раз для одной модели. Потом можно закоментить, мы будем использовать готовые файлы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb0dN4Cc2wCL"
      },
      "outputs": [],
      "source": [
        "# model = YOLO('yolov8s.pt') # инициализируем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL5Je7Q1aJY8"
      },
      "outputs": [],
      "source": [
        "# model.train(data='/content/drive/MyDrive/dataset-v1.2/data_custom.yaml', epochs=1)  # Обучаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QuV4ynCXF23b"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files  # здесь сохраняем веса от обучения\n",
        "# files.download('/content/runs/detect/train/weights/best.pt') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Эта функция позволяет пройтись по всем номерованным видео от start_vid до end_vid\n",
        "в папке video_source используя модель model_in_path находящуюся в папке path_model и \n",
        "складывая фалы с боксами от детекции (без треккера) в эту же папку где сама модель\"\"\"\n",
        "\n",
        "path_model = '/content/drive/MyDrive/best.pt/45/'           # папка с моделью куда все складываем \n",
        "model_in_path = 'best45.pt'                                 # модель\n",
        "video_source = '/content/drive/MyDrive/dataset-v1.1/test/'  # видео для детекции\n",
        "\n",
        "detect_videos(path_model, model_in_path, video_source, start_vid = 1, end_vid = 2)"
      ],
      "metadata": {
        "id": "g0cD5jpB88JP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE97uS-moLPa"
      },
      "source": [
        "## Трекккинг и постобработка"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Эта функция позволяет пройтись по всем сохраненным ранее детекциям от start_vid до end_vid \n",
        "если они существуют в папке с моделью path_model оттрековать эти боксы, наложить на соответствующие\n",
        "видео и записать результат постобработки в словари\"\"\"\n",
        "\n",
        "path_model = '/content/drive/MyDrive/best.pt/45/'   ## Модель\n",
        "tracker_path = 'ocsort00/'\n",
        "video_source ='/content/drive/MyDrive/dataset-v1.1/test/'\n",
        "\n",
        "tracker = OCSort(det_thresh = 0.49428431641933235, max_age = 7, \n",
        "                  min_hits = 7, iou_threshold = 0.6247364818234254, \n",
        "                  delta_t = 5, asso_func = 'iou', \n",
        "                  inertia = 0.6758806605183052, use_byte = True)\n",
        "\n",
        "d_in1, d_out1, d_in2, d_out2, d_helmet, d_unif = track_on_detect(path_model, tracker_path, video_source, tracker, start_vid = 1)\n",
        "\n",
        "print('Количество входящих на первом алгоритме (№ видео: кол-во): ', d_in1)\n",
        "print('Количество выходящих на первом алгоритме(№ видео: кол-во): ', d_out1)\n",
        "print('Количество входящих на втором алгоритме(№ видео: кол-во): ', d_in2)\n",
        "print('Количество выходящих на втором алгоритме(№ видео: кол-во): ', d_out2)\n",
        "print('Люди в каске 1, без каски 0 (№ видео: порядок от первого к последнему): ', d_helmet)\n",
        "print('Люди в жилете 1, без жилета 0 (№ видео: порядок от первого к последнему): ', d_unif)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WoziCus6qOP",
        "outputId": "7a5fbd49-1d1a-4df2-a8c8-25329393cdf8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "данные по видео 32: отсутствуют\n",
            "данные по видео 33: отсутствуют\n",
            "данные по видео 34: отсутствуют\n",
            "данные по видео 35: отсутствуют\n",
            "данные по видео 37: отсутствуют\n",
            "данные по видео 41: отсутствуют\n",
            "данные по видео 43: отсутствуют\n",
            "Количество входящих на первом алгоритме (№ видео: кол-во):  {'1': 3, '2': 3, '3': 3, '4': 4, '5': 0, '6': 1, '7': 0, '8': 0, '9': 1, '10': 0, '11': 0, '12': 0, '13': 2, '14': 0, '15': 2, '16': 0, '17': 0, '18': 4, '19': 1, '20': 0, '21': 1, '22': 0, '23': 0, '24': 1, '25': 2, '26': 0, '27': 1, '28': 1, '29': 1, '30': 1, '31': 0, '32': 0, '33': 0, '34': 0, '35': 0, '36': 0, '37': 0, '38': 8, '39': 0, '40': 0, '41': 0, '42': 2, '43': 0}\n",
            "Количество выходящих на первом алгоритме(№ видео: кол-во):  {'1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 4, '7': 4, '8': 5, '9': 1, '10': 2, '11': 3, '12': 4, '13': 4, '14': 3, '15': 0, '16': 3, '17': 5, '18': 0, '19': 0, '20': 0, '21': 1, '22': 3, '23': 1, '24': 0, '25': 0, '26': 1, '27': 1, '28': 1, '29': 1, '30': 3, '31': 2, '32': 0, '33': 0, '34': 0, '35': 0, '36': 0, '37': 0, '38': 7, '39': 1, '40': 1, '41': 0, '42': 0, '43': 0}\n",
            "Количество входящих на втором алгоритме(№ видео: кол-во):  {'1': 3, '2': 3, '3': 3, '4': 4, '5': 0, '6': 1, '7': 0, '8': 0, '9': 1, '10': 0, '11': 0, '12': 0, '13': 2, '14': 0, '15': 2, '16': 0, '17': 0, '18': 4, '19': 1, '20': 0, '21': 1, '22': 0, '23': 0, '24': 1, '25': 2, '26': 0, '27': 1, '28': 1, '29': 1, '30': 1, '31': 0, '32': 0, '33': 0, '34': 0, '35': 0, '36': 0, '37': 0, '38': 8, '39': 0, '40': 0, '41': 0, '42': 2, '43': 0}\n",
            "Количество выходящих на втором алгоритме(№ видео: кол-во):  {'1': 0, '2': 0, '3': 0, '4': 0, '5': 0, '6': 4, '7': 4, '8': 5, '9': 1, '10': 2, '11': 3, '12': 4, '13': 4, '14': 3, '15': 0, '16': 3, '17': 5, '18': 0, '19': 0, '20': 0, '21': 1, '22': 3, '23': 1, '24': 0, '25': 0, '26': 1, '27': 1, '28': 1, '29': 1, '30': 3, '31': 2, '32': 0, '33': 0, '34': 0, '35': 0, '36': 0, '37': 0, '38': 7, '39': 1, '40': 1, '41': 0, '42': 0, '43': 0}\n",
            "Люди в каске 1, без каски 0 (№ видео: порядок от первого к последнему):  {'1': [1, 1, 1], '2': [0, 1, 1], '3': [1, 0, 1], '4': [1, 0, 1, 1], '5': [], '6': [0, 0, 0, 0, 1], '7': [0, 0, 0, 0], '8': [0, 0, 0, 0, 0], '9': [0, 1], '10': [0, 0], '11': [0, 0, 0], '12': [0, 0, 0, 1], '13': [1, 0, 0, 0, 0, 0], '14': [1, 1, 1], '15': [0, 0], '16': [0, 1, 0], '17': [0, 0, 1, 0, 0], '18': [1, 1, 1, 1], '19': [0], '20': [], '21': [0, 0], '22': [0, 1, 0], '23': [0], '24': [1], '25': [0, 0], '26': [0], '27': [0, 1], '28': [0, 1], '29': [0, 0], '30': [0, 0, 0, 0], '31': [1, 1], '32': [], '33': [], '34': [], '35': [], '36': [], '37': [], '38': [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], '39': [1], '40': [0], '41': [], '42': [0, 0], '43': []}\n",
            "Люди в жилете 1, без жилета 0 (№ видео: порядок от первого к последнему):  {'1': [1, 1, 1], '2': [1, 1, 1], '3': [1, 0, 1], '4': [1, 0, 1, 1], '5': [], '6': [0, 0, 0, 0, 0], '7': [0, 0, 1, 0], '8': [0, 0, 0, 0, 0], '9': [1, 1], '10': [1, 1], '11': [0, 0, 1], '12': [0, 0, 0, 1], '13': [1, 1, 0, 0, 1, 1], '14': [1, 1, 1], '15': [0, 0], '16': [1, 1, 0], '17': [0, 1, 0, 1, 1], '18': [1, 1, 0, 0], '19': [1], '20': [], '21': [0, 0], '22': [0, 1, 0], '23': [0], '24': [1], '25': [0, 0], '26': [0], '27': [0, 1], '28': [0, 1], '29': [0, 0], '30': [0, 1, 1, 1], '31': [1, 1], '32': [], '33': [], '34': [], '35': [], '36': [], '37': [], '38': [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], '39': [1], '40': [0], '41': [], '42': [0, 0], '43': []}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzEuJ1SOV_RI"
      },
      "source": [
        "## Подсчет ошибок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iwWWXH92dqt3"
      },
      "outputs": [],
      "source": [
        "#Истинные значения вошедших и вышедших на видео\n",
        "dict_in_true =  {'1':3,'2':3,'3':3,'4':4,'5':0,'6':1,'7':0,'8':0,'9':1,'10':0,'11':0,'12':0,'13':2,'14':0,'15':2,'16':0,'17':0,'18':4,'19':1,'20':0,'21':1,'22':0,'23':0,'24':1,'25':2,'26':0,'27':1,'28':1,'29':1,'30':1,'31':0,'32':0,'33':0,'34':0,'35':0,'36':0,'37':0,'38':8,'39':0,'40':0,'41':0,'42':2,'43':0}  # 20 видео битое с невошедшим призраком (человек раздвоился)\n",
        "dict_out_true = {'1':0,'2':0,'3':0,'4':0,'5':0,'6':4,'7':4,'8':5,'9':1,'10':2,'11':3,'12':4,'13':4,'14':3,'15':0,'16':4,'17':6,'18':0,'19':0,'20':0,'21':1,'22':3,'23':1,'24':0,'25':0,'26':1,'27':1,'28':1,'29':1,'30':3,'31':2,'32':0,'33':0,'34':0,'35':0,'36':0,'37':0,'38':7,'39':1,'40':1,'41':0,'42':0,'43':0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mUOCmhKsWMWr"
      },
      "outputs": [],
      "source": [
        "def compare_dict(d1,d2): #функция сравнения словарей входящих/выходящих предсказанных с эталонным словарем\n",
        "  dict_all ={}\n",
        "  d_1 = set(d1.keys())\n",
        "  d_2 = set(d2.keys())\n",
        "  common_keys = d_1.intersection(d_2)\n",
        "  \n",
        "  if common_keys:\n",
        "    dict_all = {k: d1[k] - d2[k] for k in common_keys}\n",
        "  return dict_all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0uNX2nbrF_-",
        "outputId": "3ee68b5d-8fae-4691-a332-4b2b063610ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "суммарное количество ошибок первого алгоритма на входящих рассматриваемых видео 0\n",
            "в видео 17 выходящих 6 а не 5\n",
            "в видео 16 выходящих 4 а не 3\n",
            "суммарное количество ошибок первого алгоритма на выходящих рассматриваемых видео 2\n",
            "суммарное количество ошибок второго алгоритма на входящих рассматриваемых видео 0\n",
            "в видео 17 выходящих 6 а не 5\n",
            "в видео 16 выходящих 4 а не 3\n",
            "суммарное количество ошибок второго алгоритма на выходящих рассматриваемых видео 2\n"
          ]
        }
      ],
      "source": [
        "d_in_comp = compare_dict(d_in1, dict_in_true)\n",
        "for key, value in d_in_comp.items(): \n",
        "  if value != 0:\n",
        "    print(f'в видео {key} входящих {dict_in_true[key]} а не {d_in1[key]}')\n",
        "print(f'суммарное количество ошибок первого алгоритма на входящих рассматриваемых видео {sum(abs(ele) for ele in d_in_comp.values())}')\n",
        "\n",
        "d_out_comp = compare_dict(d_out1, dict_out_true)\n",
        "for key, value in d_out_comp.items(): \n",
        "  if value != 0:\n",
        "    print(f'в видео {key} выходящих {dict_out_true[key]} а не {d_out1[key]}')\n",
        "print(f'суммарное количество ошибок первого алгоритма на выходящих рассматриваемых видео {sum(abs(ele) for ele in d_out_comp.values())}')\n",
        "\n",
        "d_in_comp = compare_dict(d_in2, dict_in_true)\n",
        "for key, value in d_in_comp.items(): \n",
        "  if value != 0:\n",
        "    print(f'в видео {key} входящих {dict_in_true[key]} а не {d_in2[key]}')\n",
        "print(f'суммарное количество ошибок второго алгоритма на входящих рассматриваемых видео {sum(abs(ele) for ele in d_in_comp.values())}')\n",
        "\n",
        "d_out_comp = compare_dict(d_out2, dict_out_true)\n",
        "for key, value in d_out_comp.items(): \n",
        "  if value != 0:\n",
        "    print(f'в видео {key} выходящих {dict_out_true[key]} а не {d_out2[key]}')\n",
        "print(f'суммарное количество ошибок второго алгоритма на выходящих рассматриваемых видео {sum(abs(ele) for ele in d_out_comp.values())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJublwU0Vqlw"
      },
      "source": [
        "## Нарушения на видео"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Q2pNEEqtVpU0"
      },
      "outputs": [],
      "source": [
        "#Истинные значения нарушений на видео\n",
        "d_true_helmet = {'1': [1, 1, 1], '2': [0, 1, 1], '3': [1, 0, 1], '4': [1, 0, 1, 1], '5': [], '6': [0, 0, 0, 0, 1], '7': [0, 0, 0, 0], '8': [0, 0, 0, 0, 0], '9': [0, 1], '10': [0, 0], '11': [0, 0, 0], '12': [0, 0, 0, 1], '13': [1, 0, 0, 0, 0, 0], '14': [1, 1, 1], '15': [0, 0], '16': [0, 0, 1, 0], '17': [0, 1, 0, 0, 0, 0], '18': [1, 1, 1, 1], '19': [0], '20': [], '21': [0,0], '22': [0, 1, 0], '23': [0], '24': [1], '25': [0, 0], '26': [0], '27': [0, 1], '28': [0, 1], '29': [0, 0], '30': [0, 0, 0, 0], '31': [1, 1], '32': [], '33': [], '34': [], '35': [], '36': [], '37': [], '38': [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], '39': [1], '40': [0], '41': [], '42': [0, 0], '43': []} # 20 видео битое с невошедшим призраком (человек раздвоился)\n",
        "d_true_unif = {'1': [1, 1, 1], '2': [1, 1, 1], '3': [1, 0, 1], '4': [1, 0, 1, 1], '5': [], '6': [0, 0, 0, 0, 1], '7': [1, 0, 1, 0], '8': [0, 0, 0, 0, 0], '9': [1, 1], '10': [1, 1], '11': [0, 0, 1], '12': [0, 0, 0, 1], '13': [1, 1, 0, 0, 1, 1], '14': [1, 1, 1], '15': [0, 0], '16': [1, 1, 1, 0], '17': [0, 1, 0, 0, 1, 1], '18': [1, 1, 0, 0], '19': [1], '20': [], '21': [0,0], '22': [0, 1, 0], '23': [0], '24': [1], '25': [0, 0], '26': [0], '27': [0, 1], '28': [0, 1], '29': [0, 0], '30': [0, 1, 1, 1], '31': [1, 1], '32': [], '33': [], '34': [], '35': [], '36': [], '37': [], '38': [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1], '39': [1], '40': [0], '41': [], '42': [0, 0], '43': []} # 20 видео битое с невошедшим призраком (человек раздвоился)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liNZEkpZ7h8H",
        "outputId": "d758a8b0-14f2-4117-fa6f-92e7688df352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1': [1, 0, 0], '2': [1, 1, 0, 0, 0, 0], '3': [1, 0, 2, 2], '4': [1, 1, 0, 2]}\n",
            "{'1': [1, 1, 2], '2': [1, 1, 0, 0, 2, 2], '3': [1, 1, 0, 0], '4': [1, 1, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "\"\"\" \n",
        "Эта функция выравнивает длину массива в нарушениях по каждому видео так что бы подать в метрику sklearn массивы одной длины\n",
        "Делает она это следующим образом: \n",
        "- Создаются копии массивов с эталоном и предсказанием\n",
        "- Изменению подвергаются только те массивы (их копии) по видео в которых количество айди предсказанных отличается от эталонного !!!(их всего 1-2)!!!\n",
        "В других массивах ничего менять не нужно потому что порядок в котором айди попадают в предсказания совпадает с эталоном \n",
        "(проверено в ручную на нескольких экспериментах)\n",
        "- Копии этих массивов сортируются от 1 к 0 и там где не хватает значений добавляются маркеры отсутсвия детекции или доп детекции в случае эталона\n",
        "( двойки - это третий класс снижающий P R для наших двух нарушения/ненарушения) \n",
        "Таким образом мы выравниваем длины массивов\n",
        "\n",
        "\n",
        "Идея заключается в том что нам нужно при вычислении метрик подать пары значений и главное подать ту пару где у нас происходит отличие предсказания от эталона\n",
        "Т.е. если предсказание определило 3 человека в касках а реально их было 4, то мы добьем четвертое значение двойкой и не важно какой из них не задетектровался\n",
        "!!!Ситуация при которой два человека в эталоне по форме определились предиктом как один одетый по форме также будет штрафоваться добавлением двойки к массиву т.к. \n",
        "в противном случае это могло быть и два человека одетых по разному (один правильно - а другой нет) и обработка этого могла не заметить!!! \n",
        "\n",
        "Все обратные логики также справедливы!\n",
        "\n",
        "Подход в рамках имеющихся тестовых видео работает (проверялось вручную на нашей пособработке по всем видео)\n",
        "\n",
        "Возможно для цели подбора треккера с максимальной P метрикой по единицам логику нужно перевернуть и сортировать от меньшего к большему, добивать единицами - !Вернуться!\n",
        "\n",
        "Ниже после формулы приведена демонстрация\n",
        "\"\"\"\n",
        "\n",
        "def dict_fill(d_pred_, d_true_):  # функция которая выравнивает содержимое в предсказанном словаре с эталоно заполняя его 0\n",
        "  d_pred_exp = d_pred_.copy()  # создаем новые словари\n",
        "  d_true_exp = d_true_.copy()\n",
        "  if len(d_pred_exp) != len(d_true_exp): # если длина значений словарей не совпадает делаем соритрировку от 1 к 0 и добиваем  значения словаря меньшео размера маркером отсутсвия детекции - 2\n",
        "    d_pred_exp.sort(reverse = True)\n",
        "    d_true_exp.sort(reverse = True)\n",
        "    _list = [d_pred_exp,d_true_exp]\n",
        "    for a in _list:\n",
        "      a.extend([2] * (max(map(len, _list)) - len(a)))\n",
        "\n",
        "  return d_pred_exp, d_true_exp\n",
        "\n",
        "### Демонстрация!\n",
        "d_pred = {'1': [1, 0, 0], '2': [1, 0, 0, 0, 1, 0],  '3': [1, 0],        '4': [1, 0, 1]} # 1 - одного одетого задетектил как двоих раздетых 2 - лишние треки в середине видео 3 - пропуск отсутсвие детекции в конце 4 -объеденил в середине\n",
        "d_true = {'1': [1, 1],    '2': [1, 0, 1, 0],        '3': [1, 0, 0, 1],  '4': [1, 0, 0, 1]}\n",
        "for i in range(1,5):\n",
        "  d_pred[f'{i}'], d_true[f'{i}'] = dict_fill(d_pred[f'{i}'], d_true[f'{i}'])\n",
        "print(d_pred)\n",
        "print(d_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1uxM1xSyVWj7"
      },
      "outputs": [],
      "source": [
        "# т.к. словари при подгонке размерности видоизменяются создаем новые что бы не потерять старые \n",
        "d_true_helmet_exp = {str(n): [] for n in list(range(1,44))}\n",
        "d_helmet_exp = {str(n): [] for n in list(range(1,44))}  \n",
        "d_true_unif_exp = {str(n): [] for n in list(range(1,44))} \n",
        "d_unif_exp = {str(n): [] for n in list(range(1,44))} \n",
        "\n",
        "# Проходимся по всем словарям с нарушениями и эталонным словарям и выравниваем их\n",
        "for N in range(1,44):\n",
        "  d_helmet_exp[f'{N}'], d_true_helmet_exp[f'{N}'] = dict_fill(d_helmet[f'{N}'], d_true_helmet[f'{N}']) # выравниваем количество (длины массивов) человек для каждого видео в предсказаниях и эталонах (описание функции выше)\n",
        "  d_unif_exp[f'{N}'], d_true_unif_exp[f'{N}'] = dict_fill(d_unif[f'{N}'], d_true_unif[f'{N}']) # выравниваем количество (длины массивов) человек для каждого видео в предсказаниях и эталонах (описание функции выше)\n",
        "\n",
        "#превращаем значения словарей сначала в список списков а потом в вектор для подачи в склерн\n",
        "d_true_helmet_list = [element for innerList in list(d_true_helmet_exp.values()) for element in innerList] \n",
        "d_helmet_list = [element for innerList in list(d_helmet_exp.values()) for element in innerList] \n",
        "d_true_unif_list = [element for innerList in list(d_true_unif_exp.values()) for element in innerList] \n",
        "d_unif_list = [element for innerList in list(d_unif_exp.values()) for element in innerList] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMBHf3Fks8nI",
        "outputId": "1803fad0-68c8-4457-c99c-c44f605038f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R по каскам:  1.0\n",
            "P по каскам:  1.0\n",
            "R по униформе:  0.94\n",
            "P по униформе:  1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "precision, recall,_,_ = precision_recall_fscore_support(d_true_helmet_list, d_helmet_list)\n",
        "precision2, recall2,_,_ = precision_recall_fscore_support(d_true_unif_list, d_unif_list)\n",
        "print('R по каскам: ', round(recall[1],2))\n",
        "print('P по каскам: ', round(precision[1],2))\n",
        "print('R по униформе: ', round(recall2[1],2))\n",
        "print('P по униформе: ', round(precision2[1],2))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "GF38gPBa_TgF",
        "GE97uS-moLPa",
        "rzEuJ1SOV_RI",
        "fJublwU0Vqlw"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}